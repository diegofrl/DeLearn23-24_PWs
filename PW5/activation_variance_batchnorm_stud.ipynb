{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187bc0fb-4972-4653-a85d-6f5c2fe9c039",
   "metadata": {},
   "source": [
    "## Activation Variance\n",
    "\n",
    "This script is to illustrate the findings of X. Glorot and J. Bengio in their well known publication `Understanding the difficulty of training deep feedforward neural networks` (http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). Different initialisation schemes for the weights of a MLP can be chosen and the behaviour of the activation in the forward an backward path be observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd50d87c-2744-475b-8edc-926c241e3d6d",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09560d7d-7bbe-4dc6-929e-abda57c98b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation_histogram(num_bins=200, range = [-0.5,0.5], fig_size = [6,4]):\n",
    "    \"\"\"\n",
    "        show histogram of activations of all layers of NNet instance in one graph\n",
    "\n",
    "        Arguments:\n",
    "        num_bins -- set number of bins to use\n",
    "        range -- it is important to fix the same range (-> bins) for all curves to make them comparable\n",
    "        fig_size -- size of figure\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    ax = fig.subplots()\n",
    "\n",
    "    last = -1 #we have no reference       \n",
    "    for ind in np.arange(len(NNet.layers)):\n",
    "        if NNet.layers[ind].__class__.__name__ == 'DenseLayer':    \n",
    "            data = NNet.layers[ind].a_out.flatten()\n",
    "    \n",
    "            if last > 0:\n",
    "                var_ratio = last/np.var(data)\n",
    "                label_string = ('layer %d, r = %1.2f' % (ind, var_ratio))\n",
    "            else:\n",
    "                label_string = ('layer %d' % ind)\n",
    "           \n",
    "            ax.hist(data, bins=num_bins, range=range, histtype='step', label=label_string)\n",
    "            #store std of last layer\n",
    "            last = np.var(data)\n",
    "\n",
    "    ax.set_ylabel('absolute frequency')\n",
    "    ax.set_xlabel('activation value')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_gradient_histogram(num_bins=200, range = [-0.02,0.02], fig_size = [6,4]):\n",
    "    \"\"\"\n",
    "        show histogram of gradients of all layers of NNet instance in one graph\n",
    "\n",
    "        Arguments:\n",
    "        num_bins -- set number of bins to use\n",
    "        range -- it is important to fix the same range (-> bins) for all curves to make them comparable\n",
    "        fig_size -- size of figure\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    ax = fig.subplots()\n",
    "\n",
    "    last = -1 #we have no reference       \n",
    "    for ind in np.arange(len(NNet.layers)-1):\n",
    "        if NNet.layers[ind].__class__.__name__ == 'DenseLayer':    \n",
    "            data = NNet.layers[ind].dL_da_in.flatten()\n",
    "    \n",
    "            if last > 0:\n",
    "                var_ratio = np.var(data)/last #switched due to backprop\n",
    "                label_string = ('layer %d, r = %1.2f' % (ind, var_ratio))\n",
    "            else:\n",
    "                label_string = ('layer %d' % ind)\n",
    "           \n",
    "            ax.hist(data, bins=num_bins, range=range, histtype='step', label=label_string)\n",
    "            #store std of last layer\n",
    "            last = np.var(data)\n",
    "\n",
    "    ax.set_ylabel('absolute frequency')\n",
    "    ax.set_xlabel('gradient value')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a172d649-72cf-4f60-9e8a-32d41336f3e0",
   "metadata": {},
   "source": [
    "### Batch Normalization Layer\n",
    "\n",
    "Implementation of class for batch normalization layer (to be put after dense layer). This class is used by the class MultiLayerPerceptron below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452df94-c0b1-4b78-a8a7-c43493e7bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormLayer:\n",
    "    \"\"\"\n",
    "    constructs a batch normalization layer for a MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, size_in, eps = 1.0e-3):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        size_in -- number of inputs (i.e. of neurons from previous layer)\n",
    "        eps -- small number use to avoid division by zero (reduced for tests)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.size_in = size_in\n",
    "        self.size_out = size_in\n",
    "        \n",
    "        #average values of mu and sigma used for inference\n",
    "        self.sigma_avg = np.ones((1, size_in))\n",
    "        self.mu_avg = np.zeros((1, size_in))\n",
    "        \n",
    "        #parameters gamma and beta trained during back propagation\n",
    "        self.gamma = np.ones((1, size_in))\n",
    "        self.beta = np.zeros((1, size_in))\n",
    "        \n",
    "        self.eps = eps\n",
    "        \n",
    "        self.init = True\n",
    "        \n",
    "    def propagate(self, a_in, learn = False):\n",
    "        \"\"\"\n",
    "        calculates the activation a_out based on activation from previous layer a_in\n",
    "        \n",
    "        Arguments:\n",
    "        learn -- set to True during learning\n",
    "        \"\"\"\n",
    "        \n",
    "        if learn:\n",
    "            #in case we are learning      \n",
    "            \n",
    "            ### START YOUR CODE ###\n",
    "        \n",
    "            mu = np.zeros([1,a_in.shape[1]])\n",
    "            self.sigma = np.ones([1,a_in.shape[1]])\n",
    "            self.a_in_scal = a_in\n",
    "    \n",
    "            ### END YOUR CODE ###   \n",
    "\n",
    "            #determine average values of mu and sigma\n",
    "            if self.init == True:\n",
    "                #first training step\n",
    "                self.mu_avg = mu\n",
    "                self.sigma_avg = self.sigma\n",
    "                self.init = False\n",
    "            else:\n",
    "                beta = 0.9\n",
    "                self.mu_avg = beta*self.mu_avg + (1 - beta) * mu\n",
    "                self.sigma_avg = beta*self.sigma_avg + (1 - beta) * self.sigma\n",
    "                \n",
    "        else:\n",
    "            #for inference we scale inputs with average values of mu and sigma\n",
    "            self.a_in_scal = (a_in - self.mu_avg) / (self.eps + self.sigma_avg)\n",
    "          \n",
    "        #rescaling of the activations with gamma and beta\n",
    "        self.a_out = self.gamma * self.a_in_scal + self.beta\n",
    "        \n",
    "        return self.a_out\n",
    "            \n",
    "     \n",
    "    def back_propagate(self, dL_da_out):\n",
    "        \"\"\"\n",
    "        calculates the backpropagation results based on the gradient of cost wrt to output activations\n",
    "        this function must be performed AFTER the corresponding propagte step\n",
    "        \"\"\"        \n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        \n",
    "        #derivatives wrt gamma and beta\n",
    "        self.dL_dgamma = np.zeros([1,dL_da_out.shape[1]])\n",
    "        self.dL_dbeta = np.zeros([1,dL_da_out.shape[1]])\n",
    "        \n",
    "        #derivatives wrt input activations\n",
    "        dL_da_in = dL_da_out\n",
    "        \n",
    "        ### END YOUR CODE ###    \n",
    "        \n",
    "        return dL_da_in\n",
    "        \n",
    "        \n",
    "    def gradient_descend(self, alpha):\n",
    "        \"\"\"\n",
    "        does the gradient descend based on results from last back_prop step with learning rate alpha\n",
    "        \"\"\"\n",
    "        self.gamma -= alpha * self.dL_dgamma\n",
    "        self.beta -= alpha * self.dL_dbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14428653-6eeb-4f21-a9ac-59d411fdfbd1",
   "metadata": {},
   "source": [
    "### DenseLayerÂ¶\n",
    "\n",
    "Implementation of class for a simplified dense layer of MLP with different activation functions. This class is used by the class NeuralNetwork below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c5f301-e98a-40c4-b7b8-967b321e1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    \"\"\"\n",
    "    constructs a dense layer with for a MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, size_in, size_out, random_std = 0, activation = 'sigmoid'):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        size_in -- number of inputs (i.e. of neurons from previous layer)\n",
    "        size_out -- number of outputs (i.e. of neurons in this layer) \n",
    "        random_std -- std for initialisation of weight (default is 0)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.size_in = size_in\n",
    "        self.size_out = size_out\n",
    "        self.activation = activation\n",
    "        self.random_std = random_std\n",
    "        \n",
    "        # initialize weights and bias (zero or random)\n",
    "        self.initialise_weights()\n",
    "    \n",
    "    def initialise_weights(self):\n",
    "        \"\"\"\n",
    "        initialize weights and bias with uniform distribution\n",
    "        \"\"\" \n",
    "        self.W = self.random_std * 2*(np.random.rand(self.size_in, self.size_out)-0.5)    \n",
    "        self.B =  0*np.ones((1, self.size_out))#we apply a bias of value zero\n",
    "        \n",
    "              \n",
    "    def propagate(self, a_in, learn = False):\n",
    "        \"\"\"\n",
    "        calculates the activation a_out based on activation from previous layer a_in\n",
    "        \"\"\"    \n",
    "        self.a_in = a_in\n",
    "        self.Z = a_in @ self.W + self.B\n",
    "        self.a_out = self.activation_function(self.Z)\n",
    "        \n",
    "        return self.a_out\n",
    "\n",
    "    \n",
    "    def activation_function(self, z):\n",
    "        \"\"\"\n",
    "        apply activation function\n",
    "        \"\"\"\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1/(1+np.exp(-z))\n",
    "        if self.activation == 'tanh':\n",
    "            return (np.exp(z) - np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "        if self.activation == 'relu':\n",
    "            return z*(z > 0)\n",
    "    \n",
    "    \n",
    "    def d_activation_function(self, z):\n",
    "        \"\"\"\n",
    "        calculates the derivative of the activation function\n",
    "        \"\"\"\n",
    "        if self.activation == 'sigmoid':\n",
    "            return self.activation_function(z)*(1 - self.activation_function(z))\n",
    "        if self.activation == 'tanh':\n",
    "            return 4/np.square(np.exp(z)+np.exp(-z))\n",
    "        if self.activation == 'relu':\n",
    "            return 1.*(z > 0)\n",
    "            \n",
    "     \n",
    "    def back_propagate(self, dL_da_out):\n",
    "        \"\"\"\n",
    "        calculates the backpropagation results based on the gradient of cost wrt to output activations\n",
    "        this function must be performed AFTER the corresponding propagte step\n",
    "        \"\"\"    \n",
    "        self.dL_da_out = dL_da_out\n",
    "        dL_dz = dL_da_out*self.d_activation_function(self.Z)\n",
    "        self.dL_da_in = dL_dz @ self.W.T\n",
    "        \n",
    "        return self.dL_da_in\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13fb7f-c7a2-490d-9c06-1f7713634828",
   "metadata": {},
   "source": [
    "## Class NeuralNetwork\n",
    "\n",
    "Implementation of a MLP with configurable number of Dense Layers (only!). Only propagation and backpropagation methods are implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc8260-4492-4d72-8d19-0d5ead10b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    NN class handling the layers and doing all propagation and back-propagation steps\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, list_num_neurons, random_std = 0, activation = 'sigmoid', batch_norm = False):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        list_num_neurons -- list of neurons including in- and output layers\n",
    "        random_std -- std for initialisation of weight (default is 0)      \n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        #first construct dense layers (if any)\n",
    "        for i0 in range(len(list_num_neurons)-1):\n",
    "            new_layer = DenseLayer(list_num_neurons[i0], list_num_neurons[i0+1], random_std, activation)\n",
    "            self.layers.append(new_layer)\n",
    "\n",
    "            if batch_norm:\n",
    "                new_layer = BatchNormLayer(list_num_neurons[i0+1])\n",
    "                self.layers.append(new_layer)\n",
    "\n",
    "    \n",
    "    def propagate(self, x, learn = False):\n",
    "        \"\"\"\n",
    "        calls successively propagate methods of all layers \n",
    "        result of each layer is passed to next layer\n",
    "        \"\"\"\n",
    "        for layers in self.layers:\n",
    "            x = layers.propagate(x, learn = learn)\n",
    "            \n",
    "        return x\n",
    "\n",
    "     \n",
    "    def back_propagate(self, y):\n",
    "        \"\"\"\n",
    "        calls successively back_propagate methods of all layers \n",
    "        result of each layer is passed to next layer\n",
    "        \"\"\"\n",
    "        #first do softmax with y as input\n",
    "        dL_da_out = y\n",
    "        #now the remaining layers\n",
    "        for i0 in reversed(range(len(self.layers))):\n",
    "            dL_da_out = self.layers[i0].back_propagate(dL_da_out)\n",
    "            \n",
    "        return dL_da_out\n",
    "\n",
    "    \n",
    "    def print_layers(self):\n",
    "        ind = 1\n",
    "        for layers in self.layers:\n",
    "            print('layer [%r] is %r' % (ind, layers.__class__.__name__))\n",
    "            print('#input is %r, #output is %r' % (layers.size_in, layers.size_out))\n",
    "            if layers.__class__.__name__ == 'DenseLayer':                \n",
    "                print('activation function is %r' % layers.activation)\n",
    "                print('weight init std is %r' % layers.random_std)\n",
    "            ind += 1\n",
    "            print('')\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea6a5c-c3b4-42a5-9442-c572833dde77",
   "metadata": {},
   "source": [
    "## Network initialisation\n",
    "\n",
    "Create a set of dummy input vectors x of given size and instantiate a network with given number of hidden layers and neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489f33b-e27f-4541-a719-6a8a4e427212",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "num_feat = 100\n",
    "num_classes = 100 #we choose the output to be of same size to have identical number of activations\n",
    "\n",
    "#to avoid saturation in the first layer we choose a somewhat smaller standard deviation\n",
    "stand_dev = 0.3\n",
    "x = stand_dev*np.random.randn(num_samples, num_feat)\n",
    "\n",
    "list_num_neurons = [num_feat, num_feat, num_feat, num_feat, num_feat, num_feat, num_classes]\n",
    "#note the possible choice of activation function\n",
    "\n",
    "### START YOUR CODE ###\n",
    "NNet = NeuralNetwork(list_num_neurons, random_std = 1*np.sqrt(1/num_feat), activation = 'tanh', batch_norm = False)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "NNet.print_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0adb4-934f-4574-bafc-52400fcd9fdb",
   "metadata": {},
   "source": [
    "## Propagation step\n",
    "\n",
    "Observe in detail the development of the histogram of activations and gradients across the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05898cc9-77c2-44fb-9f56-ad0ec5ba2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = NNet.propagate(x, True)\n",
    "NNet.back_propagate(y)\n",
    "\n",
    "#you may have to adjust the disply range to optimise the graphical view\n",
    "plot_activation_histogram(range = [-0.5,0.5])\n",
    "plot_gradient_histogram(range = [-0.02,0.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98a6d4-1279-4942-bc67-3ea1f32f08e6",
   "metadata": {},
   "source": [
    "# Unit Tests\n",
    "### Unit Test for BatchNormLayer propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca07b01-2e96-499f-9dea-4bd7caef2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([0.2,0.1,-0.3, 0.2, 0.5,-1.0, 1.0,1.5,0.1,-0.3, 0.2,-1.0]).reshape(4,3)\n",
    "\n",
    "\n",
    "#set seed to have reproducible weight and bias values\n",
    "np.random.seed(1)\n",
    "batchNormLayer = BatchNormLayer(3)\n",
    "\n",
    "#choose non-trivial values for gamma and beta \n",
    "batchNormLayer.gamma = np.array([[1.2, 0.3, -1.4]])\n",
    "batchNormLayer.beta = np.array([[-0.2, 1.8, 0.4]])\n",
    "\n",
    "#do propagation\n",
    "y_pred = batchNormLayer.propagate(x_0, True)\n",
    "\n",
    "#compare with expected result\n",
    "y_exp = np.array([[-0.39284403,  1.54322612, -0.34042881],\n",
    "       [-0.39284403,  1.75945676,  1.73277186],\n",
    "       [ 1.66415895,  2.30003334, -1.52511491],\n",
    "       [-1.67847089,  1.59728378,  1.73277186]]\n",
    ")\n",
    "\n",
    "mu_avg_exp = np.array([[ 0.275,  0.575, -0.55 ]])\n",
    "sigma_avg_exp = np.array([[0.4656984 , 0.553963  , 0.47169906]])\n",
    "\n",
    "np.testing.assert_array_almost_equal(y_pred,y_exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(batchNormLayer.mu_avg,mu_avg_exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(batchNormLayer.sigma_avg,sigma_avg_exp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee98527-51fa-4a9b-95aa-8b3fb449dda2",
   "metadata": {},
   "source": [
    "### Unit Test for BatchNormLayer back_propagate\n",
    "Assumes that Unit Test for propagate is correctÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a65d89-d864-49f9-a751-0ec5c305739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([0.2,0.1,-0.3, 0.2, 0.5,-1.0, 1.0,1.5,0.1,-0.3, 0.2,-1.0]).reshape(4,3)\n",
    "\n",
    "\n",
    "#set seed to have reproducible weight and bias values\n",
    "np.random.seed(1)\n",
    "batchNormLayer = BatchNormLayer(3)\n",
    "\n",
    "#choose non-trivial values for gamma and beta \n",
    "batchNormLayer.gamma = np.array([[1.2, 0.3, -1.4]])\n",
    "batchNormLayer.beta = np.array([[-0.2, 1.8, 0.4]])\n",
    "\n",
    "#do propagation\n",
    "y_pred = batchNormLayer.propagate(x_0, True)\n",
    "dL_da = batchNormLayer.back_propagate(y_pred)\n",
    "\n",
    "dL_dbeta_exp = np.array([[-0.2, 1.8, 0.4]])\n",
    "dL_dgamma_exp = np.array([[ 1.194863  ,  0.29891982, -1.39408284]])\n",
    "dL_da_exp = np.array([[-2.12265436e-03, -4.99784266e-04,  9.26855963e-03],\n",
    "       [-2.12265436e-03, -7.89133052e-05, -1.66834073e-02],\n",
    "       [ 2.05189921e-02,  9.73264097e-04,  2.40982550e-02],\n",
    "       [-1.62736834e-02, -3.94566526e-04, -1.66834073e-02]])\n",
    "\n",
    "np.testing.assert_array_almost_equal(dL_da,dL_da_exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(batchNormLayer.dL_dbeta,dL_dbeta_exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(batchNormLayer.dL_dgamma,dL_dgamma_exp,decimal=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef0d80-bcd8-46c8-afd3-34959e0d9813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
