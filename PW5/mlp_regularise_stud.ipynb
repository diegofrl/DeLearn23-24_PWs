{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expressed-suffering",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP)\n",
    "\n",
    "Multi-class classification problem - using a MLP with configurable number of hidden neurons - with a configurable number of classes (up to 10). It selects them from the (Fashion-)MNIST dataset, splits it up into a train and test part, does normalisation and then trains a classifier using softmax.\n",
    "\n",
    "Both datasets consist of images with 28x28 = 784 pixel each. The features refer to these pixel values of the images.\n",
    "\n",
    "You can choose MNIST or Fashion-MNIST data in cell [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9ac65-cf2d-4571-93ce-aeb172cbfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only at first execution data is downloaded, because it is saved in subfolder ../week1/data; \n",
    "#note the relative path to the 01.learning-optimization to avoid multiple downloads\n",
    "data_set = 'FashionMNIST'\n",
    "    \n",
    "if data_set == 'FahsionMNIST':\n",
    "    training_data = torchvision.datasets.MNIST(\n",
    "        root=\"../01.learning-optimization/data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    test_data = torchvision.datasets.MNIST(\n",
    "        root=\"../01.learning-optimization/data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )    \n",
    "\n",
    "    #labels for MNIST (just for compatibility reasons)\n",
    "    labels_map = {\n",
    "        0: \"Zero\",\n",
    "        1: \"One\",\n",
    "        2: \"Two\",\n",
    "        3: \"Three\",\n",
    "        4: \"Four\",\n",
    "        5: \"Five\",\n",
    "        6: \"Six\",\n",
    "        7: \"Seven\",\n",
    "        8: \"Eight\",\n",
    "        9: \"Nine\",\n",
    "    }\n",
    "else:\n",
    "    training_data = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../01.learning-optimization/data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    test_data = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../01.learning-optimization/data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    #labels for FashionMNIST\n",
    "    labels_map = {\n",
    "        0: \"T-Shirt\",\n",
    "        1: \"Trouser\",\n",
    "        2: \"Pullover\",\n",
    "        3: \"Dress\",\n",
    "        4: \"Coat\",\n",
    "        5: \"Sandal\",\n",
    "        6: \"Shirt\",\n",
    "        7: \"Sneaker\",\n",
    "        8: \"Bag\",\n",
    "        9: \"Ankle Boot\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed1556-c895-4ab6-be00-c0bc4a660abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy array (originally it is a torch.tensor)\n",
    "x = training_data.data.numpy()\n",
    "x = np.append(x, test_data.data.numpy(),0)\n",
    "\n",
    "y = training_data.targets.numpy()\n",
    "y = np.append(y, test_data.targets.numpy())\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img):\n",
    "    \"\"\"\n",
    "    plot a single mnist image\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img, cmap=plt.cm.gray)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    \n",
    "plot_img(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tiles(x_array, rows, cols = -1, fig_size = [10,10]):\n",
    "    \"\"\"\n",
    "    plot list of images as single image\n",
    "\n",
    "    Arguments:\n",
    "    x_array -- array of images (being organised as ROWS!)\n",
    "    rows/cols -- an image of rows x cols - images is created (if x_array is smaller zeros ared padded)\n",
    "    fig_size -- size of full image created (default [10,10])\n",
    "    \"\"\"\n",
    "\n",
    "    digit_size = 28 #size of digit (width = height)\n",
    "    \n",
    "    #use rows = cols as default\n",
    "    if cols < 0:\n",
    "        cols = rows\n",
    "        \n",
    "    if x_array.shape[0] < rows*cols:\n",
    "        cols = int(x_array.shape[0]/rows)\n",
    "        remain = np.mod(x_array.shape[0], rows)\n",
    "        if 0 < remain:\n",
    "            cols += 1\n",
    "            x_array = np.append(x_array, np.zeros((rows-remain, x_array.shape[1])), 0)    \n",
    "        \n",
    "    img = x_array[0:rows,:].reshape(rows*digit_size,digit_size)\n",
    "    for i0 in range(1,cols):\n",
    "        #the reshape operator in the append call takes num of digit_size x digit_size images and \n",
    "        #puts them in a single column; append then does the rest\n",
    "        img = np.append(img, x_array[i0*rows:(i0+1)*rows,:].reshape(rows*digit_size,digit_size),1)\n",
    "\n",
    "    fig = plt.figure(figsize = fig_size)\n",
    "    ax = fig.subplots()\n",
    "    ax.imshow(img, cmap=plt.cm.gray)\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append rows x cols tiles of images\n",
    "rows = 8\n",
    "cols = 18\n",
    "#figure size can be set\n",
    "fig_size = [8,8]\n",
    "\n",
    "plot_tiles(x, rows, cols, fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a given class 0..9\n",
    "digit  = 0\n",
    "\n",
    "plot_tiles(x[y == digit,:], rows, cols, fig_size)\n",
    "print(labels_map[digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7f320-89df-4cd2-a795-1b4fcee72901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the classes for your training and test set, select train and test split and to normalization\n",
    "def prepare_data(classes, train_size=0.8, min_max_normalise=1, flatten=1):\n",
    "    \"\"\"\n",
    "    prepare the data for training\n",
    "\n",
    "    Arguments:\n",
    "    classes -- list of classes to use for training (at least two classes must be given)\n",
    "    train_size -- fraction of train image size\n",
    "    min_max_normalise -- whether to do min-max-normalisation (1) or rescaling (0)\n",
    "    flatten -- whether to flatten the 28x28 image to single row (=1); otherwise a new dimension is added at axis=1 (to be compatible with cnn)\n",
    "    \"\"\"\n",
    "\n",
    "    if len(classes) < len(labels_map):\n",
    "        for label in classes:\n",
    "            print('labels chosen are: %r' % labels_map[label])\n",
    "\n",
    "    ind_sel = np.isin(y, classes)\n",
    "    x_sel = x[ind_sel,:].copy()\n",
    "    y_sel = y[ind_sel].copy()\n",
    "\n",
    "    #replace the labels such that they are in successive order\n",
    "    for i0 in range(0,len(classes)):\n",
    "        if i0 != classes[i0]:\n",
    "            y_sel[y_sel == classes[i0]] = i0\n",
    "\n",
    "    #we give y back as simple vector -> simplifies handling below\n",
    "    #y_sel = np.reshape(y_sel, (-1,1))\n",
    "    \n",
    "    #do train and test split\n",
    "    num_samples = x_sel.shape[0]\n",
    "    max_train_ind = int(train_size*num_samples)\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    x_train = x_sel[indices[:max_train_ind]]\n",
    "    x_test = x_sel[indices[max_train_ind:]]\n",
    "    \n",
    "    y_train = y_sel[indices[:max_train_ind]]\n",
    "    y_test = y_sel[indices[max_train_ind:]]\n",
    "\n",
    "    #perform normalisation, take care of converting data type to float!\n",
    "    xmax, xmin = np.max(x_train), np.min(x_train)\n",
    "    \n",
    "    if min_max_normalise:\n",
    "        x_train = 2*(x_train.astype(float) - xmin) / (xmax - xmin) - 1\n",
    "        x_test = 2*(x_test.astype(float) - xmin) / (xmax - xmin) - 1\n",
    "    else:\n",
    "        x_train = x_train.astype(float) / xmax \n",
    "        x_test = x_test.astype(float) / xmax \n",
    "\n",
    "    if flatten:\n",
    "        m = x_train.shape[0]\n",
    "        x_train = x_train.reshape([m,-1])\n",
    "        m = x_test.shape[0]\n",
    "        x_test = x_test.reshape([m,-1])\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cce1b9-2a6a-47dd-b93b-8fb8b527fd08",
   "metadata": {},
   "source": [
    "### Class MiniBatches\n",
    "\n",
    "Splits the given dataset (`x: features` and `y: labels`) into individual batches of size `batch_size` (a value of `0` will return the full batch). The total number of batches available in an epoch is returned with method `number_of_batches()`. Each call to `next()` will return a new batch in the given format: `{'x_batch': x_batch, 'y_batch': y_batch}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c698c3d-05aa-4662-b8e6-1c9065793bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatches:\n",
    "    \"\"\"\n",
    "    split set into batches\n",
    "\n",
    "    Arguments:\n",
    "    x -- features\n",
    "    y -- corresponding labels\n",
    "    batch_size -- size of batches\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        m = x.shape[0]\n",
    "        self.indices = np.arange(m)\n",
    "        self.n = x.shape[1]\n",
    "        \n",
    "        if not batch_size:\n",
    "            self.batch_size = m\n",
    "            self.mb = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size        \n",
    "            self.mb = int(m / self.batch_size)    \n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "        self.ib = 0\n",
    "\n",
    "    def number_of_batches(self):\n",
    "        return self.mb\n",
    "\n",
    "    def next(self):\n",
    "        it = self.indices[self.ib * self.batch_size:(self.ib + 1) * self.batch_size]\n",
    "        x_batch = self.x[it, :]\n",
    "        y_batch = self.y[it]\n",
    "        self.ib += 1\n",
    "\n",
    "        return {'x_batch': x_batch, 'y_batch': y_batch}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2200343-87a5-4a7a-90d1-218d501dd1cb",
   "metadata": {},
   "source": [
    "### DropoutLayer\n",
    "Implementation of class for dropout layer with given dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b620d-23f4-4445-aa67-0c3734fd40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutLayer:\n",
    "    \"\"\"\n",
    "    constructs a dropout layer with given dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, size_in, drop_p = 0.5):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "        \n",
    "        Arguments:\n",
    "        size_in -- number of inputs (i.e. of neurons from previous layer); equals number of outputs\n",
    "        drop_p -- dropout probability\n",
    "        \"\"\"\n",
    "        self.size_in = size_in\n",
    "        self.size_out = size_in\n",
    "        self.drop_p = drop_p;\n",
    "    \n",
    "    \n",
    "    def set_dropout_vector(self, len_v):\n",
    "        \"\"\"\n",
    "        calculates a vector of 0/1 with length len_v for dropout of current step\n",
    "        fraction of 0 should be equal to self.drop_p; ordering random\n",
    "        \"\"\"\n",
    "### START YOUR CODE ###            \n",
    "        self.drop_out = np.ones([1,len_v])\n",
    "### END YOUR CODE ###            \n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def propagate(self, a_in, learn = False):\n",
    "        \"\"\"\n",
    "        applies dropout\n",
    "        \"\"\"    \n",
    "        if a_in.shape[1] != self.size_in:\n",
    "            print('warning: size %r does not correspond to initialisation %r' % (a_in.shape[1], self.size_in))\n",
    "        \n",
    "        self.a_out = a_in\n",
    "        #apply dropout\n",
    "        if learn:\n",
    "            self.set_dropout_vector(self.a_out.shape[1])\n",
    "            self.a_out = self.a_out*self.drop_out/(1-self.drop_p) #broadcast\n",
    "        \n",
    "        return self.a_out\n",
    "            \n",
    "     \n",
    "    def back_propagate(self, dL_da_out):\n",
    "        \"\"\"\n",
    "        applies dropout\n",
    "        \"\"\"    \n",
    "### START YOUR CODE ###             \n",
    "        #apply dropout in backward path\n",
    "        self.dL_da_in = dL_da_out\n",
    "### END YOUR CODE ###     \n",
    "        \n",
    "        return self.dL_da_in\n",
    "    \n",
    "    def gradient_descend(self, alpha):\n",
    "        \"\"\"\n",
    "        dummy implementation to satisfy interface\n",
    "        \"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcec2a5-61e3-484b-94d8-88bf60f83674",
   "metadata": {},
   "source": [
    "### DenseLayer¶\n",
    "\n",
    "Implementation of class for dense layer of MLP with sigmoid activation function. This class is used by the class NeuralNetwork below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53a7d4-57ba-4c71-8dcd-88dbe6676a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    \"\"\"\n",
    "    constructs a dense layer with for a MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, size_in, size_out, activation = 'sigmoid', regularize = 0):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        size_in -- number of inputs (i.e. of neurons from previous layer)\n",
    "        size_out -- number of outputs (i.e. of neurons in this layer) \n",
    "        regularize -- L2 regularisation factor\n",
    "        \"\"\"\n",
    "        \n",
    "        self.size_in = size_in\n",
    "        self.size_out = size_out\n",
    "        self.activation = activation\n",
    "        self.regularize = regularize\n",
    "\n",
    "        # initialize weights and bias (zero or random)\n",
    "        self.initialise_weights()\n",
    "    \n",
    "    def initialise_weights(self):\n",
    "        \"\"\"\n",
    "        initialize weights and bias with Glorot or He scheme\n",
    "        \"\"\" \n",
    "### START YOUR CODE ###        \n",
    "        #correct scaling for given activation function\n",
    "        if self.activation == 'sigmoid':\n",
    "            random_std_W = 0    \n",
    "        if self.activation == 'tanh':\n",
    "            random_std_W = 0   \n",
    "        if self.activation == 'relu':\n",
    "            random_std_W = 0     \n",
    "### END YOUR CODE ###        \n",
    "        \n",
    "        self.W = random_std_W * 2*(np.random.rand(self.size_in, self.size_out)-0.5)  \n",
    "        random_std_B = 0 #set to zero\n",
    "        self.B = random_std_B * 2*(np.random.rand(1, self.size_out)-0.5)\n",
    "        \n",
    "              \n",
    "    def propagate(self, a_in, learn = False):\n",
    "        \"\"\"\n",
    "        calculates the activation a_out based on activation from previous layer a_in\n",
    "        \"\"\"    \n",
    "        self.num_samples = a_in.shape[0] #may change in different steps\n",
    "        self.a_in = a_in # required for back_prop\n",
    "        self.Z = a_in @ self.W + self.B\n",
    "        self.a_out = self.activation_function(self.Z)\n",
    "        \n",
    "        return self.a_out\n",
    "\n",
    "    \n",
    "    def activation_function(self, z):\n",
    "        \"\"\"\n",
    "        apply activation function\n",
    "        \"\"\"\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1/(1+np.exp(-z))\n",
    "### START YOUR CODE ###            \n",
    "        if self.activation == 'tanh':\n",
    "            return z\n",
    "        if self.activation == 'relu':\n",
    "            return z\n",
    "### END YOUR CODE ###    \n",
    "    \n",
    "    def d_activation_function(self, z):\n",
    "        \"\"\"\n",
    "        calculates the derivative of the activation function\n",
    "        \"\"\"\n",
    "        if self.activation == 'sigmoid':\n",
    "            return self.activation_function(z)*(1 - self.activation_function(z))\n",
    "### START YOUR CODE ###            \n",
    "        if self.activation == 'tanh':\n",
    "            return z\n",
    "        if self.activation == 'relu':\n",
    "            return z\n",
    "### END YOUR CODE ###           \n",
    "     \n",
    "    def back_propagate(self, dL_da_out):\n",
    "        \"\"\"\n",
    "        calculates the backpropagation results based on the gradient of cost wrt to output activations\n",
    "        this function must be performed AFTER the corresponding propagte step\n",
    "        \"\"\"    \n",
    "        dL_dz = dL_da_out*self.d_activation_function(self.Z)\n",
    "        self.dL_dW = (self.a_in.T @ dL_dz) / self.num_samples #number of samples from previous prop step\n",
    "        self.dL_dB = np.sum(dL_dz, 0, keepdims=True) / self.num_samples\n",
    "        dL_da_in = dL_dz @ self.W.T\n",
    "        \n",
    "        return dL_da_in\n",
    "        \n",
    "        \n",
    "    def gradient_descend(self, alpha):\n",
    "        \"\"\"\n",
    "        does the gradient descend based on results from last back_prop step with learning rate alpha\n",
    "        \"\"\"\n",
    "### START YOUR CODE ###        \n",
    "        self.W -= alpha * self.dL_dW\n",
    "        self.B -= alpha * self.dL_dB\n",
    "### END YOUR CODE ###\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b75677e-fae2-42d3-a487-e2ef527a0b35",
   "metadata": {},
   "source": [
    "### SoftmaxLayer\n",
    "Implementation of class for softmax layer of MultiLayerPerceptron below. This class is used by the class NeuralNetwork below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd8c8a-81ca-4488-956c-6dcd1e936452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    \"\"\"\n",
    "    constructs a sofmax layer for output of a MLP\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size_in, size_out, activation = 'sigmoid', regularize = 0):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        size_in -- number of inputs from previous layer\n",
    "        size_out -- number of neurons i.e. if outputs\n",
    "        regularize -- L2 regularisation factor\n",
    "        \"\"\"\n",
    "        \n",
    "        self.size_in = size_in\n",
    "        self.size_out = size_out\n",
    "        self.activation = activation\n",
    "        self.regularize = regularize\n",
    "         \n",
    "        # initialize weights and bias (zero or random)\n",
    "        self.initialise_weights()\n",
    "    \n",
    "    \n",
    "    def initialise_weights(self):\n",
    "        \"\"\"\n",
    "        initialize weights and bias with Glorot or He scheme (we use sigmoid scheme because it is closest)\n",
    "        \"\"\" \n",
    "        random_std_W = 4*np.sqrt(6.0/(self.size_in + self.size_out)) # suited for sigmod activation function     \n",
    "        random_std_B = 0 #in case we want to introduce a bias\n",
    "\n",
    "        self.W = random_std_W * 2*(np.random.rand(self.size_in, self.size_out)-0.5)    \n",
    "        self.B = random_std_B * 2*(np.random.rand(1, self.size_out)-0.5)\n",
    "        \n",
    "        \n",
    "    def propagate(self, a_in, learn = False):\n",
    "        \"\"\"\n",
    "        calculates the activation a_out based on activation from previous layer a_in\n",
    "        \"\"\"    \n",
    "        self.num_samples = a_in.shape[0] #may change in different steps\n",
    "        self.a_in = a_in # required for back_prop\n",
    "        self.Z = a_in @ self.W + self.B\n",
    "\n",
    "        #clip large values\n",
    "        max_val = 700.\n",
    "        self.Z = np.clip(self.Z, -max_val, max_val)\n",
    "            \n",
    "        exp_Z = np.exp(self.Z)\n",
    "        self.y_pred = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "        \n",
    "        return self.y_pred\n",
    "    \n",
    "    \n",
    "    def one_hot(self, y):\n",
    "        \"\"\"\n",
    "        construct onehot vector from set of labels\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        one_hot = np.zeros((m, self.size_out), dtype=float)\n",
    "        one_hot[np.arange(m), y] = 1\n",
    "\n",
    "        return one_hot\n",
    "            \n",
    "     \n",
    "    def back_propagate(self, y):\n",
    "        \"\"\"\n",
    "        calculates the backpropagation results based on expected output y\n",
    "        this function must be performed AFTER the corresponding propagte step\n",
    "        \"\"\"    \n",
    "        dL_dz = self.y_pred - self.one_hot(y)\n",
    "            \n",
    "        self.dL_dW = (self.a_in.T @ dL_dz) / self.num_samples #number of samples from previous prop step\n",
    "        self.dL_dB = np.sum(dL_dz, 0, keepdims=True) / self.num_samples\n",
    "        dL_da_in = dL_dz @ self.W.T\n",
    "        \n",
    "        return dL_da_in\n",
    "        \n",
    "        \n",
    "    def gradient_descend(self, alpha):\n",
    "        \"\"\"\n",
    "        does the gradient descend based on results from last back_prop step with learning rate alpha\n",
    "        \"\"\"\n",
    "### START YOUR CODE ###        \n",
    "        self.W -= alpha * self.dL_dW\n",
    "        self.B -= alpha * self.dL_dB\n",
    "### END YOUR CODE ###\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcd5fb-d25e-4fe5-906e-386c955898a1",
   "metadata": {},
   "source": [
    "### Class NeuralNetwork\n",
    "\n",
    "This class constructs a Multilayer Perceptron with a configurable number of hidden layers. Cost function is CE. The method $propagate()$ returns the prediction $$ \\hat{y}^{(i)}=h_\\theta(\\mathbf{x}^{(i)}) $$ on the input data (can be a n x 784 matrix of n images) and $back\\_propagate()$ determines the gradients of the cost function with respect to the parameters (weights and bias for all layers) $$ \\nabla_{\\mathbf{\\theta}} J(\\mathbf{\\theta}) $$\n",
    "The method $gradient\\_descend()$ finally does the correction of the parameters with a step in the negative gradient direction, weighted with the learning rate $$\\alpha$$ for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c21d2-2815-4e6c-b359-6b0f26eac64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    NN class handling the layers and doing all propagation and back-propagation steps\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, list_num_neurons, activation = 'sigmoid', regularize = 0, drop_p = 0):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        list_num_neurons -- list of neurons including in- and output layers\n",
    "        regularize -- L2 regularisation parameter\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        #first construct dense layers (if any)\n",
    "        for i0 in range(len(list_num_neurons)-2):\n",
    "            if drop_p > 0: #add dropout layer\n",
    "                new_layer = DropoutLayer(list_num_neurons[i0], drop_p)\n",
    "                self.layers.append(new_layer)\n",
    "                \n",
    "            new_layer = DenseLayer(list_num_neurons[i0], list_num_neurons[i0+1], activation, regularize)\n",
    "            self.layers.append(new_layer)\n",
    "\n",
    "        if drop_p > 0: #add final dropout layer\n",
    "            new_layer = DropoutLayer(list_num_neurons[-2], drop_p)\n",
    "            self.layers.append(new_layer)\n",
    "        \n",
    "        #finally add softmax layer\n",
    "        new_layer = SoftmaxLayer(list_num_neurons[-2], list_num_neurons[-1], activation, regularize)\n",
    "        self.layers.append(new_layer)\n",
    "\n",
    "        # result array\n",
    "        self.result_data = np.array([])\n",
    "        \n",
    "        #we keep a global step counter, thus that optimise can be called \n",
    "        #several times with different settings\n",
    "        self.epoch_counter = 0 \n",
    "\n",
    "    \n",
    "    def propagate(self, x, learn = False):\n",
    "        \"\"\"\n",
    "        calls successively propagate methods of all layers \n",
    "        result of each layer is passed to next layer\n",
    "        dropout layer requires knowledge about learning or not\n",
    "        \"\"\"\n",
    "        for layers in self.layers:\n",
    "            x = layers.propagate(x, learn = learn)\n",
    "            \n",
    "        return x\n",
    "\n",
    "     \n",
    "    def back_propagate(self, y):\n",
    "        \"\"\"\n",
    "        calls successively back_propagate methods of all layers \n",
    "        result of each layer is passed to next layer\n",
    "        \"\"\"\n",
    "        #first do softmax with y as input\n",
    "        dL_da_out = y\n",
    "        #now the remaining layers\n",
    "        for i0 in reversed(range(len(self.layers))):\n",
    "            dL_da_out = self.layers[i0].back_propagate(dL_da_out)\n",
    "            \n",
    "        return dL_da_out\n",
    "    \n",
    "    \n",
    "    def gradient_descend(self, alpha):\n",
    "        \"\"\"\n",
    "        calls successively gradient_descend methods of all layers \n",
    "        \"\"\"\n",
    "        for layers in self.layers:\n",
    "            layers.gradient_descend(alpha)  \n",
    "    \n",
    "    \n",
    "    def calc_error(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        get error information\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        \n",
    "        y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "        error = np.sum(y != y_pred_argmax) / m\n",
    "\n",
    "        return error\n",
    "\n",
    "\n",
    "    def one_hot(self, y):\n",
    "        \"\"\"\n",
    "        construct onehot vector from set of labels\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        one_hot = np.zeros((m, self.size_out), dtype=float)\n",
    "        one_hot[np.arange(m), y] = 1\n",
    "\n",
    "        return one_hot\n",
    "    \n",
    "    \n",
    "    def cost_funct(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        calculates the cost function\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        \n",
    "        #take care of possible over over or underflow\n",
    "        eps=1.0e-12\n",
    "        y_pred = np.clip(y_pred,eps,1-eps)\n",
    "        py = y_pred[np.arange(m), y]\n",
    "        cost = -np.sum(np.log(py)) / m\n",
    "        \n",
    "        return cost   \n",
    "    \n",
    "    \n",
    "    def append_result(self):\n",
    "        \"\"\"\n",
    "        append cost and error data to output array\n",
    "        \"\"\"\n",
    "        # determine cost and error functions for train and validation data\n",
    "        y_pred_train = self.propagate(self.data['x_train'])\n",
    "        y_pred_val = self.propagate(self.data['x_val'])\n",
    "\n",
    "        res_data = np.array([[self.cost_funct(y_pred_train, self.data['y_train']), \n",
    "                              self.calc_error(y_pred_train, self.data['y_train']),\n",
    "                              self.cost_funct(y_pred_val, self.data['y_val']), \n",
    "                              self.calc_error(y_pred_val, self.data['y_val'])]])\n",
    "        \n",
    "        # first call\n",
    "        if self.result_data.size == 0:\n",
    "            self.result_data = res_data\n",
    "        else:\n",
    "            self.result_data = np.append(self.result_data, res_data, 0)\n",
    "\n",
    "        #increase epoch counter here (used for plot routines below)\n",
    "        self.epoch_counter += 1 \n",
    "        \n",
    "        return res_data\n",
    "\n",
    "\n",
    "    def print_layers(self):\n",
    "        ind = 1\n",
    "        for layers in self.layers:\n",
    "            print('layer [%r] is %r' % (ind, layers.__class__.__name__))\n",
    "            print('#input is %r, #output is %r' % (layers.size_in, layers.size_out))\n",
    "            if layers.__class__.__name__ == 'DropoutLayer':\n",
    "                print('dropout rate is %r\\n' % layers.drop_p)\n",
    "            else:                \n",
    "                print('activation function is %r' % layers.activation)\n",
    "                print('regularisation is %r\\n' % layers.regularize)\n",
    "            ind += 1\n",
    "          \n",
    "    def optimise(self, data, epochs, alpha, batch_size=0, debug=0):\n",
    "        \"\"\"\n",
    "        performs epochs number of gradient descend steps and appends result to output array\n",
    "\n",
    "        Arguments:\n",
    "        data -- dictionary with NORMALISED data\n",
    "        epochs -- number of epochs\n",
    "        alpha -- learning rate\n",
    "        batch_size -- size of batches (1 = SGD, 0 = batch, 1 < .. < n = mini-batch)\n",
    "        debug -- integer value; get info on gradient descend step every debug-step (0 -> no output)\n",
    "        \"\"\"\n",
    "        #access to data from other methods\n",
    "        self.data = data\n",
    "        \n",
    "        # save results before 1st step\n",
    "        if self.epoch_counter == 0:\n",
    "            res_data = self.append_result()\n",
    "\n",
    "        for i0 in range(0, epochs):    \n",
    "            # create batches for each epoch\n",
    "            batches = MiniBatches(self.data['x_train'], self.data['y_train'], batch_size)\n",
    "\n",
    "            for ib in range(batches.number_of_batches()):\n",
    "                batch = batches.next()\n",
    "            \n",
    "                y_pred = self.propagate(batch['x_batch'], learn = True)\n",
    "                self.back_propagate(batch['y_batch'])\n",
    "                self.gradient_descend(alpha)\n",
    "          \n",
    "            res_data = self.append_result()\n",
    "                      \n",
    "            if debug and np.mod(i0, debug) == 0:\n",
    "                print('result after %d epochs, train: cost %.5f, error %.5f ; validation: cost %.5f, error %.5f'\n",
    "                                          % (self.epoch_counter-1, res_data[0, 0], res_data[0, 1], res_data[0, 2], res_data[0, 3]))\n",
    "\n",
    "        if debug:\n",
    "            print('result after %d epochs, train: cost %.5f, error %.5f ; validation: cost %.5f, error %.5f'\n",
    "                  % (self.epoch_counter-1, res_data[0, 0], res_data[0, 1], res_data[0, 2], res_data[0, 3]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(nn_instance):\n",
    "    \"\"\"\n",
    "    analyse error as function of epochs\n",
    "\n",
    "    Arguments:\n",
    "    nn_instance -- NeuralNetwork class to plot\n",
    "    \"\"\"\n",
    "    epochs = np.arange(nn_instance.epoch_counter)\n",
    "    train_error = nn_instance.result_data[:,1]\n",
    "    val_error = nn_instance.result_data[:,3]\n",
    "\n",
    "    plt.semilogy(epochs, train_error, label=\"train\")\n",
    "    plt.semilogy(epochs, val_error, label=\"validation\")\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epochs')\n",
    "    xmax = epochs[-1]\n",
    "    ymin = 1e-3\n",
    "    ymax = 5e-1\n",
    "    plt.axis([0,xmax,ymin,ymax])\n",
    "    plt.legend()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost(nn_instance):\n",
    "    \"\"\"\n",
    "    analyse cost as function of epochs\n",
    "\n",
    "    Arguments:\n",
    "    nn_instance -- NeuralNetwork class to plot\n",
    "    \"\"\"\n",
    "    epochs = np.arange(nn_instance.epoch_counter)\n",
    "    train_costs = nn_instance.result_data[:,0]\n",
    "    val_costs = nn_instance.result_data[:,2]\n",
    "\n",
    "    plt.semilogy(epochs, train_costs, label=\"train\")\n",
    "    plt.semilogy(epochs, val_costs, label=\"validation\")\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Epochs')\n",
    "    xmax = epochs[-1]\n",
    "    ymin = 1e-2\n",
    "    ymax = 2\n",
    "    plt.axis([0,xmax,ymin,ymax])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59684055-267e-4163-84e3-8ddf07a59331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_hist(nn_inst, num_bins = 50, fig_size=[10,5]):\n",
    "    \"\"\"\n",
    "    plots the parameter distribution (weights and biases) for all layers as histogram\n",
    "    \"\"\"      \n",
    "    for layer in nn_inst.layers:\n",
    "        if layer.__class__.__name__ == 'DenseLayer' or layer.__class__.__name__ == 'SoftmaxLayer':\n",
    "            fig, axs = plt.subplots(1, 2, figsize=fig_size)\n",
    "            key = layer.__class__.__name__\n",
    "    \n",
    "            weight =layer.W.reshape(-1)\n",
    "            bias = layer.B.reshape(-1)\n",
    "            # We can set the number of bins with the *bins* keyword argument.\n",
    "            axs[0].hist(weight, bins=num_bins)\n",
    "            axs[0].set_title(key + ' - weights')\n",
    "            axs[0].set_ylabel('count')\n",
    "            axs[0].set_xlabel('value')\n",
    "            axs[1].hist(bias, bins=num_bins)\n",
    "            axs[1].set_title(key + ' - biases')\n",
    "            axs[1].set_ylabel('count (bias)')\n",
    "            axs[1].set_xlabel('value')\n",
    "    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfae34-9c4a-461f-b06f-d9abc093b775",
   "metadata": {},
   "source": [
    "### Sample execution of Neural Network\n",
    "\n",
    "The cell below shows how to use the class NeuralNetwork and how to perform the optimisation. The training and test data is given as dictionary in the call to the method $optimise()$. The classes (from 2 to 10) can be chosen via the `classes` list. This method can be called several times in a row with different arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346948e-7b50-4aa5-9444-da2076d4fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the categories\n",
    "classes = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "x_train, x_test, y_train, y_test = prepare_data(classes, train_size=0.8, min_max_normalise=0, flatten=1)\n",
    "\n",
    "#further split in train and validation data\n",
    "validation_size = 0.2\n",
    "valid_ind = int(x_train.shape[0]*(1-validation_size))\n",
    "\n",
    "#data is arranged as dictionary with quick access through respective keys\n",
    "data = {'x_train' : x_train[:valid_ind,:], 'y_train' : y_train[:valid_ind],  \\\n",
    "               'x_val' : x_train[valid_ind:,:], 'y_val' : y_train[valid_ind:]}\n",
    "\n",
    "#choose the hyperparameters you want to use for the initialisation\n",
    "size_in = x_train.shape[1]\n",
    "size_out = len(classes)\n",
    "list_num_neurons = [size_in, 200, 100, size_out]; \n",
    "NNet = NeuralNetwork(list_num_neurons, activation = 'sigmoid', regularize = 0.0, drop_p = 0.0)\n",
    "\n",
    "NNet.print_layers()\n",
    "\n",
    "#choose the hyperparameters you want to use for training\n",
    "epochs = 150\n",
    "batchsize = 16\n",
    "learning_rate = 0.1\n",
    "NNet.optimise(data, epochs, learning_rate, batchsize, debug=5)\n",
    "\n",
    "\n",
    "plot_error(NNet)\n",
    "plot_cost(NNet)\n",
    "\n",
    "plot_parameter_hist(NNet)\n",
    "\n",
    "y_pred = np.argmax(NNet.propagate(x_test), axis=1)\n",
    "false_classifications = x_test[(y_pred != y_test)]\n",
    "\n",
    "print('test error rate: %.2f %% out of %d' % (100*false_classifications.shape[0]/y_pred.shape[0], y_pred.shape[0]))\n",
    "print(false_classifications.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b7882-711e-4aea-8e93-6aedd179c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse false classified training or test images\n",
    "y_pred = np.argmax(NNet.propagate(x_test), axis=1)\n",
    "false_classifications = x_test[(y_pred != y_test)]\n",
    "\n",
    "print('test error rate: %.2f %% out of %d' % (100*false_classifications.shape[0]/y_pred.shape[0], y_pred.shape[0]))\n",
    "print(false_classifications.shape)\n",
    "\n",
    "#append rows x cols tiles of digits\n",
    "rows = 7\n",
    "cols = 8\n",
    "#figure size can be set\n",
    "fig_size = [8,8]\n",
    "\n",
    "plot_tiles(false_classifications, rows, cols, fig_size)\n",
    "\n",
    "#print the correct labels (for FashionMNIST)\n",
    "if rows*cols < false_classifications.shape[0]:\n",
    "    false_classifications_y = y_test[y_pred != y_test][:rows*cols]\n",
    "else:\n",
    "    false_classifications_y = np.append(y_test[y_pred != y_test], np.ones(rows*cols - false_classifications.shape[0])*-1)\n",
    "print(false_classifications_y.reshape([cols,rows]).T.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee00a7a-f1d3-45df-8ea9-b52abdf583c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise weights of the first layer\n",
    "\n",
    "#search first dense layer\n",
    "found = False\n",
    "ind = 0\n",
    "while not found and ind < len(NNet.layers):\n",
    "    if NNet.layers[ind].__class__.__name__ == 'DenseLayer':\n",
    "        found = True\n",
    "        firstDense = ind\n",
    "    ind += 1\n",
    "\n",
    "        \n",
    "print('we have %r weight vectors in layer [%r]' % (NNet.layers[firstDense].W.shape[1], firstDense))\n",
    "print('choose a suitable combination of rows and cols below to plot them')\n",
    "\n",
    "rows = 5\n",
    "cols = 20\n",
    "#figure size can be set\n",
    "fig_size = [14,6]\n",
    "\n",
    "plot_tiles(NNet.layers[firstDense].W.T, rows, cols, fig_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a857e1-a96e-41a1-b898-929ccb6bc455",
   "metadata": {},
   "source": [
    "# Tests\n",
    "### Test analytical value of derivative using backpropagation through comparison with difference quotient (does not work for Dropout layers!!)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50570663-c447-4e15-844d-6514642758b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define set of m dummy samples with num_features features each being N(0,1) normalised\n",
    "num_features = 50\n",
    "m = 3\n",
    "num_outcomes = 4\n",
    "x_0 = np.random.randn(m, num_features)\n",
    "y_0 = np.random.randint(0,num_outcomes,size=m)\n",
    "\n",
    "#create a dummy MLP; num of input and output layers is fixed, hidden layers can be configured\n",
    "NNet = NeuralNetwork([num_features, 50, 40, 30,num_outcomes], activation = 'sigmoid', regularize = 0.0)\n",
    "\n",
    "NNet.print_layers()\n",
    "\n",
    "#do propagation and backpropagation\n",
    "y_pred = NNet.propagate(x_0)\n",
    "dL_x = NNet.back_propagate(y_0)\n",
    "\n",
    "L = NNet.cost_funct(y_pred, y_0)\n",
    "\n",
    "#apply eps to given layer\n",
    "eps = 1.0e-8\n",
    "#maximum allowed difference\n",
    "max_diff = 4.0e-7\n",
    "\n",
    "#loop over all layers\n",
    "layer_ind = 0\n",
    "for layer in NNet.layers:\n",
    "    print('test weights W for layer %r' % layer_ind)\n",
    "    #loop over all weights in layer\n",
    "    for i0 in range(layer.W.shape[0]):\n",
    "        for i1 in range(layer.W.shape[1]):\n",
    "            #add eps to weight\n",
    "            layer.W[i0,i1] += eps\n",
    "            #do propagation\n",
    "            y_pred = NNet.propagate(x_0)\n",
    "            #determine new cost function and ...\n",
    "            L_eps = NNet.cost_funct(y_pred, y_0)\n",
    "            #determine approx. derivative\n",
    "            dL = (L_eps-L)/eps\n",
    "\n",
    "            #compare this value to result from backpropagation\n",
    "            if np.abs(dL - layer.dL_dW[i0,i1]) > max_diff:\n",
    "                print('layer = %r, i0 = %r, i1 = %r, diff = %r' %(layer_ind, i0, i1, dL-layer.dL_dW[i0,i1]))\n",
    "\n",
    "            #set value back to original value\n",
    "            layer.W[i0,i1] -= eps\n",
    "\n",
    "    print('test bias B for layer %r' % layer_ind)\n",
    "    #loop over all bias values in layer\n",
    "    for i1 in range(layer.B.shape[1]):\n",
    "        #add eps to weight\n",
    "        layer.B[0,i1] += eps\n",
    "        #do propagation\n",
    "        y_pred = NNet.propagate(x_0)\n",
    "        #determine new cost function and ...\n",
    "        L_eps = NNet.cost_funct(y_pred, y_0)\n",
    "        #determine approx. derivative\n",
    "        dL = (L_eps-L)/eps\n",
    "\n",
    "        #compare this value to result from backpropagation\n",
    "        if np.abs(dL - layer.dL_dB[0,i1]) > max_diff:\n",
    "            print('layer = %r, i0 = %r, i1 = %r, diff = %r' %(layer_ind, i0, i1, dL-dW))\n",
    "\n",
    "        #set value back to original value\n",
    "        layer.B[0,i1] -= eps\n",
    "            \n",
    "    layer_ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24646c1c-80ba-4639-b1c9-81643e8f5f2c",
   "metadata": {},
   "source": [
    "### Unit Test for Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aedf05-1139-4956-8b87-37470d358bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in = 20\n",
    "num_samples = 10\n",
    "p = 0.3\n",
    "dropoutLayer = DropoutLayer(size_in, drop_p = p)\n",
    "\n",
    "#fix seed\n",
    "np.random.seed(1)\n",
    "\n",
    "#num_samples samples with size_in features each\n",
    "a_in = np.random.randn(num_samples, size_in)\n",
    "\n",
    "a_out = dropoutLayer.propagate(a_in, True)\n",
    "a_ret = dropoutLayer.back_propagate(a_in)\n",
    "\n",
    "np.testing.assert_array_almost_equal(a_in.shape, a_out.shape)\n",
    "np.testing.assert_array_almost_equal(np.unique(dropoutLayer.drop_out), np.array([0,1]))\n",
    "np.testing.assert_array_almost_equal(np.sum(dropoutLayer.drop_out), (1-p)*size_in)\n",
    "np.testing.assert_array_almost_equal(a_out*(1-p), a_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15a674-3e3d-4785-b9bb-ecb7c3445de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
