{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expressed-suffering",
   "metadata": {},
   "source": [
    "## Generalised Perceptron\n",
    "\n",
    "Multi-class classification based on MNIST and Fashion MNIST data. \n",
    "\n",
    "Multi-class classification problem with a configurable number of classes (up to 10). It selects them from the (Fashion-)MNIST dataset, splits it up into a train and test part, does normalisation and then trains a classifier using softmax.\n",
    "\n",
    "Both datasets consist of images with 28x28 = 784 pixel each. The features refer to these pixel values of the images.\n",
    "\n",
    "You can choose MNIST or Fashion-MNIST data in cell [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9ac65-cf2d-4571-93ce-aeb172cbfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only at first execution data is downloaded, because it is saved in subfolder ../week1/data; \n",
    "#note the relative path to the 01.learning-optimization to avoid multiple downloads\n",
    "data_set = 'FashionMNIST'\n",
    "    \n",
    "if data_set == 'MNIST':\n",
    "    training_data = torchvision.datasets.MNIST(\n",
    "        root=\"../01.learning-optimization/data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    test_data = torchvision.datasets.MNIST(\n",
    "        root=\"../01.learning-optimization/data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )    \n",
    "\n",
    "    #labels for MNIST (just for compatibility reasons)\n",
    "    labels_map = {\n",
    "        0: \"Zero\",\n",
    "        1: \"One\",\n",
    "        2: \"Two\",\n",
    "        3: \"Three\",\n",
    "        4: \"Four\",\n",
    "        5: \"Five\",\n",
    "        6: \"Six\",\n",
    "        7: \"Seven\",\n",
    "        8: \"Eight\",\n",
    "        9: \"Nine\",\n",
    "    }\n",
    "else:\n",
    "    training_data = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../01.learning-optimization/data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    test_data = torchvision.datasets.FashionMNIST(\n",
    "        root=\"../01.learning-optimization/data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    #labels for FashionMNIST\n",
    "    labels_map = {\n",
    "        0: \"T-Shirt\",\n",
    "        1: \"Trouser\",\n",
    "        2: \"Pullover\",\n",
    "        3: \"Dress\",\n",
    "        4: \"Coat\",\n",
    "        5: \"Sandal\",\n",
    "        6: \"Shirt\",\n",
    "        7: \"Sneaker\",\n",
    "        8: \"Bag\",\n",
    "        9: \"Ankle Boot\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed1556-c895-4ab6-be00-c0bc4a660abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy array (originally it is a torch.tensor)\n",
    "x = training_data.data.numpy()\n",
    "x = np.append(x, test_data.data.numpy(),0)\n",
    "\n",
    "y = training_data.targets.numpy()\n",
    "y = np.append(y, test_data.targets.numpy())\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img):\n",
    "    \"\"\"\n",
    "    plot a single mnist image\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img, cmap=plt.cm.gray)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    \n",
    "plot_img(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tiles(x_array, rows, cols = -1, fig_size = [10,10]):\n",
    "    \"\"\"\n",
    "    plot list of images as single image\n",
    "\n",
    "    Arguments:\n",
    "    x_array -- array of images (being organised as ROWS!)\n",
    "    rows/cols -- an image of rows x cols - images is created (if x_array is smaller zeros ared padded)\n",
    "    fig_size -- size of full image created (default [10,10])\n",
    "    \"\"\"\n",
    "\n",
    "    digit_size = 28 #size of digit (width = height)\n",
    "    \n",
    "    #use rows = cols as default\n",
    "    if cols < 0:\n",
    "        cols = rows\n",
    "        \n",
    "    if x_array.shape[0] < rows*cols:\n",
    "        cols = int(x_array.shape[0]/rows)\n",
    "        remain = np.mod(x_array.shape[0], rows)\n",
    "        if 0 < remain:\n",
    "            cols += 1\n",
    "            x_array = np.append(x_array, np.zeros((rows-remain, x_array.shape[1])), 0)    \n",
    "        \n",
    "    img = x_array[0:rows,:].reshape(rows*digit_size,digit_size)\n",
    "    for i0 in range(1,cols):\n",
    "        #the reshape operator in the append call takes num of digit_size x digit_size images and \n",
    "        #puts them in a single column; append then does the rest\n",
    "        img = np.append(img, x_array[i0*rows:(i0+1)*rows,:].reshape(rows*digit_size,digit_size),1)\n",
    "\n",
    "    fig = plt.figure(figsize = fig_size)\n",
    "    ax = fig.subplots()\n",
    "    ax.imshow(img, cmap=plt.cm.gray)\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append rows x cols tiles of images\n",
    "rows = 8\n",
    "cols = 18\n",
    "#figure size can be set\n",
    "fig_size = [8,8]\n",
    "\n",
    "plot_tiles(x, rows, cols, fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a given class 0..9\n",
    "digit  = 0\n",
    "\n",
    "plot_tiles(x[y == digit,:], rows, cols, fig_size)\n",
    "print(labels_map[digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7f320-89df-4cd2-a795-1b4fcee72901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the classes for your training and test set, select train and test split and to normalization\n",
    "def prepare_data(classes, train_size=0.8, min_max_normalise=1, flatten=1):\n",
    "    \"\"\"\n",
    "    prepare the data for training\n",
    "\n",
    "    Arguments:\n",
    "    classes -- list of classes to use for training (at least two classes must be given)\n",
    "    train_size -- fraction of train image size\n",
    "    min_max_normalise -- whether to do min-max-normalisation (1) or rescaling (0)\n",
    "    flatten -- whether to flatten the 28x28 image to single row (=1); otherwise a new dimension is added at axis=1 (to be compatible with cnn)\n",
    "    \"\"\"\n",
    "    \n",
    "    for label in classes:\n",
    "        print('labels chosen are: %r' % labels_map[label])\n",
    "\n",
    "    ind_sel = np.isin(y, classes)\n",
    "    x_sel = x[ind_sel,:].copy()\n",
    "    y_sel = y[ind_sel].copy()\n",
    "\n",
    "    #replace the labels such that they are in successive order\n",
    "    for i0 in range(0,len(classes)):\n",
    "        if i0 != classes[i0]:\n",
    "            y_sel[y_sel == classes[i0]] = i0\n",
    "\n",
    "    #we give y back as simple vector -> simplifies handling below\n",
    "    #y_sel = np.reshape(y_sel, (-1,1))\n",
    "    \n",
    "    #do train and test split\n",
    "    num_samples = x_sel.shape[0]\n",
    "    max_train_ind = int(train_size*num_samples)\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    x_train = x_sel[indices[:max_train_ind]]\n",
    "    x_test = x_sel[indices[max_train_ind:]]\n",
    "    \n",
    "    y_train = y_sel[indices[:max_train_ind]]\n",
    "    y_test = y_sel[indices[max_train_ind:]]\n",
    "\n",
    "    #perform normalisation, take care of converting data type to float!\n",
    "    xmax, xmin = np.max(x_train), np.min(x_train)\n",
    "    \n",
    "    if min_max_normalise:\n",
    "        x_train = 2*(x_train.astype(float) - xmin) / (xmax - xmin) - 1\n",
    "        x_test = 2*(x_test.astype(float) - xmin) / (xmax - xmin) - 1\n",
    "    else:\n",
    "        x_train = x_train.astype(float) / xmax \n",
    "        x_test = x_test.astype(float) / xmax \n",
    "\n",
    "    if flatten:\n",
    "        m = x_train.shape[0]\n",
    "        x_train = x_train.reshape([m,-1])\n",
    "        m = x_test.shape[0]\n",
    "        x_test = x_test.reshape([m,-1])\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cce1b9-2a6a-47dd-b93b-8fb8b527fd08",
   "metadata": {},
   "source": [
    "### Class MiniBatches\n",
    "\n",
    "Splits the given dataset (`x: features` and `y: labels`) into individual batches of size `batch_size` (a value of `0` will return the full batch). The total number of batches available in an epoch is returned with method `number_of_batches()`. Each call to `next()` will return a new batch in the given format: `{'x_batch': x_batch, 'y_batch': y_batch}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c698c3d-05aa-4662-b8e6-1c9065793bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatches:\n",
    "    \"\"\"\n",
    "    split set into batches\n",
    "\n",
    "    Arguments:\n",
    "    x -- features\n",
    "    y -- corresponding labels\n",
    "    batch_size -- size of batches\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        m = x.shape[0]\n",
    "        self.indices = np.arange(m)\n",
    "        self.n = x.shape[1]\n",
    "        \n",
    "        if not batch_size:\n",
    "            self.batch_size = m\n",
    "            self.mb = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size        \n",
    "            self.mb = int(m / self.batch_size)    \n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "        self.ib = 0\n",
    "\n",
    "    def number_of_batches(self):\n",
    "        return self.mb\n",
    "\n",
    "    def next(self):\n",
    "        it = self.indices[self.ib * self.batch_size:(self.ib + 1) * self.batch_size]\n",
    "        x_batch = self.x[it, :]\n",
    "        y_batch = self.y[it]\n",
    "        self.ib += 1\n",
    "\n",
    "        return {'x_batch': x_batch, 'y_batch': y_batch}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcd5fb-d25e-4fe5-906e-386c955898a1",
   "metadata": {},
   "source": [
    "### Class NeuralNetwork\n",
    "\n",
    "This class constructs a generalised perceptron. Cost function is CE. The method $propagate()$ returns the prediction $$ \\hat{y}^{(i)}=h_\\theta(\\mathbf{x}^{(i)}) $$ on the input data (can be a n x 784 matrix of n images) and $back\\_propagate()$ determines the gradients of the cost function with respect to the parameters (weights and bias) $$ \\nabla_{\\mathbf{\\theta}} J(\\mathbf{\\theta}) $$\n",
    "The method $gradient\\_descend()$ finally does the correction of the parameters with a step in the negative gradient direction, weighted with the learning rate $$\\alpha$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c21d2-2815-4e6c-b359-6b0f26eac64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    NN class handling the layers and doing all propagation and back-propagation steps\n",
    "    \"\"\"\n",
    "    def __init__(self, size_out, random_std = 0, size_in = 784):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        size_out -- number of outputs (i.e. of classes)\n",
    "        random_std -- std for initialisation of weight (default is 0)\n",
    "        size_in -- size of input image\n",
    "        \"\"\"\n",
    "        self.size_out = size_out\n",
    "        self.size_in = size_in        \n",
    "\n",
    "        #initialize weights and bias (zero or random)\n",
    "        self.initialise_weights(random_std)\n",
    "        \n",
    "        # result array for cost and error of training and validation set\n",
    "        self.result_data = np.array([])\n",
    "        self.result_data_dL = np.array([])\n",
    "        \n",
    "        #we keep a global step counter, thus that optimise can be called \n",
    "        #several times with different settings\n",
    "        self.epoch_counter = 0 \n",
    "\n",
    "        \n",
    "    def initialise_weights(self, random_std):\n",
    "        \"\"\"\n",
    "        initialize weights and bias (if random_std == 0 all weights are zero)\n",
    "        \"\"\" \n",
    "        self.w = random_std*np.random.randn(self.size_in, self.size_out)\n",
    "        self.b = random_std * np.random.randn(1, self.size_out)\n",
    "\n",
    "    \n",
    "    def propagate(self, x):\n",
    "        \"\"\"\n",
    "        predicted outcome for x\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ### \n",
    "        y_pred = np.ones((x.shape[0], self.size_out))\n",
    "        ### END YOUR CODE ### \n",
    "                    \n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    def activation_function(self, z):\n",
    "        \"\"\"\n",
    "        apply activation function\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ### \n",
    "        soft_max = np.exp(z)\n",
    "        ### END YOUR CODE ### \n",
    "\n",
    "        return soft_max\n",
    "\n",
    "    \n",
    "    def one_hot(self, y):\n",
    "        \"\"\"\n",
    "        construct onehot vector from set of labels\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ### \n",
    "        m = y.shape[0]\n",
    "        one_hot = np.zeros((m, self.size_out), dtype=float)\n",
    "        one_hot[:,0] = 1\n",
    "        ### END YOUR CODE ### \n",
    "\n",
    "        return one_hot\n",
    "    \n",
    "    \n",
    "    def back_propagate(self, x, y_pred, y):\n",
    "        \"\"\"\n",
    "        calculates the gradients of cost function wrt w and b\n",
    "        \"\"\"\n",
    "        #abbreviation\n",
    "        m = x.shape[0]\n",
    "\n",
    "        ### START YOUR CODE ### \n",
    "        self.grad_w = np.zeros((self.size_in, self.size_out))\n",
    "        self.grad_b = np.zeros((1, self.size_out))\n",
    "        ### END YOUR CODE ### \n",
    "    \n",
    "    def gradient_descend(self, alpha):\n",
    "        \"\"\"\n",
    "        applies gradient descend step to w and b\n",
    "        \"\"\"\n",
    "        self.w -= alpha * self.grad_w\n",
    "        self.b -= alpha * self.grad_b\n",
    "    \n",
    "    \n",
    "    def calc_error(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        get error information\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        \n",
    "        y_pred_argmax = np.argmax(y_pred, axis=1)\n",
    "        error = np.sum(y != y_pred_argmax) / m\n",
    "\n",
    "        return error\n",
    "    \n",
    "    \n",
    "    def cost_funct(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        calculates the cost function\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        \n",
    "        #take care of possible over over or underflow\n",
    "        eps=1.0e-12\n",
    "        y_pred = np.clip(y_pred,eps,1-eps)\n",
    "        ### START YOUR CODE ### \n",
    "        cost = 0.5\n",
    "        ### END YOUR CODE ### \n",
    "                            \n",
    "        return cost   \n",
    "    \n",
    "    \n",
    "    def append_result(self):\n",
    "        \"\"\"\n",
    "        append cost and error data to output array\n",
    "        \"\"\"\n",
    "        # determine cost and error functions for train and validation data\n",
    "        y_pred_train = self.propagate(self.data['x_train'])\n",
    "        y_pred_val = self.propagate(self.data['x_val'])\n",
    "\n",
    "        res_data = np.array([[self.cost_funct(y_pred_train, self.data['y_train']), \n",
    "                              self.calc_error(y_pred_train, self.data['y_train']),\n",
    "                              self.cost_funct(y_pred_val, self.data['y_val']), \n",
    "                              self.calc_error(y_pred_val, self.data['y_val'])]])\n",
    "        \n",
    "        # first call\n",
    "        if self.result_data.size == 0:\n",
    "            self.result_data = res_data\n",
    "        else:\n",
    "            self.result_data = np.append(self.result_data, res_data, 0)\n",
    "\n",
    "        #increase epoch counter here (used for plot routines below)\n",
    "        self.epoch_counter += 1 \n",
    "        \n",
    "        return res_data\n",
    "    \n",
    "    \n",
    "          \n",
    "    def optimise(self, data, epochs, alpha, batch_size=0, debug=0):\n",
    "        \"\"\"\n",
    "        performs epochs number of gradient descend steps and appends result to output array\n",
    "\n",
    "        Arguments:\n",
    "        data -- dictionary with NORMALISED data\n",
    "        epochs -- number of epochs\n",
    "        alpha -- learning rate\n",
    "        batch_size -- size of batches (1 = SGD, 0 = batch, 1 < .. < n = mini-batch)\n",
    "        debug -- False (default)/True; get info on gradient descend step\n",
    "        \"\"\"\n",
    "        #access to data from other methods\n",
    "        self.data = data\n",
    "        \n",
    "        # save results before 1st step\n",
    "        if self.epoch_counter == 0:\n",
    "            res_data = self.append_result()\n",
    "\n",
    "        for i0 in range(0, epochs):    \n",
    "            # create batches for each epoch\n",
    "            batches = MiniBatches(self.data['x_train'], self.data['y_train'], batch_size)\n",
    "\n",
    "            for ib in range(batches.number_of_batches()):\n",
    "                batch = batches.next()\n",
    "            \n",
    "                y_pred = self.propagate(batch['x_batch'])\n",
    "                self.back_propagate(batch['x_batch'], y_pred, batch['y_batch'])\n",
    "                self.gradient_descend(alpha)\n",
    "          \n",
    "            res_data = self.append_result()\n",
    "                      \n",
    "            if debug and np.mod(i0, debug) == 0:\n",
    "                print('result after %d epochs, train: cost %.5f, error %.5f ; validation: cost %.5f, error %.5f'\n",
    "                                          % (self.epoch_counter-1, res_data[0, 0], res_data[0, 1], res_data[0, 2], res_data[0, 3]))\n",
    "\n",
    "        if debug:\n",
    "            print('result after %d epochs, train: cost %.5f, error %.5f ; validation: cost %.5f, error %.5f'\n",
    "                  % (self.epoch_counter-1, res_data[0, 0], res_data[0, 1], res_data[0, 2], res_data[0, 3]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(nn_instance):\n",
    "    \"\"\"\n",
    "    analyse error as function of epochs\n",
    "\n",
    "    Arguments:\n",
    "    nn_instance -- NeuralNetwork class to plot\n",
    "    \"\"\"\n",
    "    epochs = np.arange(nn_instance.epoch_counter)\n",
    "    train_error = nn_instance.result_data[:,1]\n",
    "    val_error = nn_instance.result_data[:,3]\n",
    "\n",
    "    plt.semilogy(epochs, train_error, label=\"train\")\n",
    "    plt.semilogy(epochs, val_error, label=\"validation\")\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epochs')\n",
    "    xmax = epochs[-1]\n",
    "    ymin = 5e-3\n",
    "    ymax = 5e-1\n",
    "    plt.axis([0,xmax,ymin,ymax])\n",
    "    plt.legend()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost(nn_instance):\n",
    "    \"\"\"\n",
    "    analyse cost as function of epochs\n",
    "\n",
    "    Arguments:\n",
    "    nn_instance -- NeuralNetwork class to plot\n",
    "    \"\"\"\n",
    "    epochs = np.arange(nn_instance.epoch_counter)\n",
    "    train_costs = nn_instance.result_data[:,0]\n",
    "    val_costs = nn_instance.result_data[:,2]\n",
    "\n",
    "    plt.semilogy(epochs, train_costs, label=\"train\")\n",
    "    plt.semilogy(epochs, val_costs, label=\"validation\")\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Epochs')\n",
    "    xmax = epochs[-1]\n",
    "    ymin = 5e-2\n",
    "    ymax = 2\n",
    "    plt.axis([0,xmax,ymin,ymax])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfae34-9c4a-461f-b06f-d9abc093b775",
   "metadata": {},
   "source": [
    "### Sample execution of Neural Network\n",
    "\n",
    "The cell below shows how to use the class NeuralNetwork and how to perform the optimisation. The training and test data is given as dictionary in the call to the method $optimise()$. The classes (from 2 to 10) can be chosen via the `classes` list. This method can be called several times in a row with different arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e5245-456f-4026-8978-8a1b8776ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the categories\n",
    "classes = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "x_train, x_test, y_train, y_test = prepare_data(classes, train_size=0.8, min_max_normalise=0, flatten=1)\n",
    "\n",
    "#further split in train and validation data\n",
    "validation_size = 0.2\n",
    "valid_ind = int(x_train.shape[0]*(1-validation_size))\n",
    "\n",
    "#data is arranged as dictionary with quick access through respective keys\n",
    "data = {'x_train' : x_train[:valid_ind,:], 'y_train' : y_train[:valid_ind],  \\\n",
    "               'x_val' : x_train[valid_ind:,:], 'y_val' : y_train[valid_ind:]}\n",
    "\n",
    "#output size may change\n",
    "size_out = len(classes)\n",
    "NNet = NeuralNetwork(size_out, random_std = 0., size_in = 784)\n",
    "\n",
    "batchsize = 256\n",
    "NNet.optimise(data, 50, 0.05, batchsize, debug=5)\n",
    "#NNet.optimise(data, 200, 0.2, debug=50)\n",
    "\n",
    "plot_error(NNet)\n",
    "plot_cost(NNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b7882-711e-4aea-8e93-6aedd179c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse false classified training or test images\n",
    "y_pred = np.argmax(NNet.propagate(x_test), axis=1)\n",
    "false_classifications = x_test[(y_pred != y_test)]\n",
    "\n",
    "print('test error rate: %.2f %% out of %d' % (100*false_classifications.shape[0]/y_pred.shape[0], y_pred.shape[0]))\n",
    "print(false_classifications.shape)\n",
    "\n",
    "#append rows x cols tiles of digits\n",
    "rows = 7\n",
    "cols = 8\n",
    "#figure size can be set\n",
    "fig_size = [8,8]\n",
    "\n",
    "plot_tiles(false_classifications, rows, cols, fig_size)\n",
    "\n",
    "#print the correct labels (for FashionMNIST)\n",
    "if rows*cols < false_classifications.shape[0]:\n",
    "    false_classifications_y = y_test[y_pred != y_test][:rows*cols]\n",
    "else:\n",
    "    false_classifications_y = np.append(y_test[y_pred != y_test], np.ones(rows*cols - false_classifications.shape[0])*-1)\n",
    "print(false_classifications_y.reshape([cols,rows]).T.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee00a7a-f1d3-45df-8ea9-b52abdf583c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise the weights, order is:\n",
    "#0 2 4 6 8\n",
    "#1 3 5 7 9 \n",
    "\n",
    "rows = 2\n",
    "cols = 5\n",
    "#figure size can be set\n",
    "fig_size = [10,6]\n",
    "\n",
    "plot_tiles(NNet.w.T, rows, cols, fig_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822e8df-a5be-41ce-94c0-35914213b5e2",
   "metadata": {},
   "source": [
    "# Unit Tests\n",
    "### Unit Test for activation_function¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ac6a7-08f9-4825-9a5f-91a7343c7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy instance\n",
    "size_out = 3\n",
    "dummyNN = NeuralNetwork(size_out)\n",
    "\n",
    "z_h = np.array([[0.468, 0.468, 0.063], [0.013, 0.265, 0.721], [0.211, 0.576, 0.211], [0.013, 0.265, 0.721]])\n",
    "\n",
    "a_h = dummyNN.activation_function(z_h)\n",
    "\n",
    "a_exp = np.array([[0.37495639, 0.37495639, 0.25008722],\n",
    "               [0.23166792, 0.29806303, 0.47026905],\n",
    "               [0.29065424, 0.41869151, 0.29065424],\n",
    "               [0.23166792, 0.29806303, 0.47026905]])\n",
    "\n",
    "np.testing.assert_array_almost_equal(a_h,a_exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(np.sum(a_h, axis=1), np.ones(4,dtype='float'), decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ee8ce-9045-4542-8c95-862290d3067a",
   "metadata": {},
   "source": [
    "### Unit Test for propagate \n",
    "Assumes that unit test for activation_function ist correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a093c-c909-4197-b568-7ea2d4db1e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy instance\n",
    "size_out = 3\n",
    "dummyNN = NeuralNetwork(size_out)\n",
    "\n",
    "dummyNN.w = np.array([[ 1,  0, -1],[-1,  1,  1]]).reshape(2,3)\n",
    "dummyNN.b = np.array([0,0,0]).reshape(1,3)\n",
    "x_h = np.array([2, 3]).reshape(1,2)\n",
    "y_pred_h = dummyNN.propagate(x_h)\n",
    "y_exp = np.array([0.01587624,0.86681333,0.11731043]).reshape(y_pred_h.shape)\n",
    "np.testing.assert_array_almost_equal(y_pred_h,y_exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(np.sum(y_pred_h, axis=1), 1.0, decimal=8)\n",
    "\n",
    "x_h = np.array([[ 2,  1],[-1,  1],[ 1,  1],[-1,  1]]).reshape(4,2)\n",
    "y_pred_h = dummyNN.propagate(x_h)\n",
    "y_exp = np.array([[0.46831053, 0.46831053, 0.06337894],\n",
    "       [0.01321289, 0.26538793, 0.72139918],\n",
    "       [0.21194156, 0.57611688, 0.21194156],\n",
    "       [0.01321289, 0.26538793, 0.72139918]]\n",
    ")\n",
    "np.testing.assert_array_almost_equal(y_pred_h,y_exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(np.sum(y_pred_h, axis=1), np.ones(4,dtype='float'), decimal=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da832b-ee56-4499-8183-a4e04eeb3d65",
   "metadata": {},
   "source": [
    "### Unit Test for one_hot¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b50bf-8c1c-4dc1-bc44-45421bc13a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy instance\n",
    "size_out = 4\n",
    "dummyNN = NeuralNetwork(size_out)\n",
    "\n",
    "y_h = np.array([1,3,0])\n",
    "one_hot_h = dummyNN.one_hot(y_h)\n",
    "one_hot_exp = np.array([[0,1,0,0],[0,0,0,1],[1,0,0,0]])\n",
    "np.testing.assert_almost_equal(one_hot_h,one_hot_exp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b1757-e38e-40e3-a8a9-ffc0cc7426ec",
   "metadata": {},
   "source": [
    "### Unit Test for back_propagate \n",
    "Assumes that unit test for propagate ist correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e1487-9c4e-442d-bc85-04fadaa7f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy instance\n",
    "size_out = 3\n",
    "dummyNN = NeuralNetwork(size_out)\n",
    "\n",
    "dummyNN.w = np.array([[ 1,  0, -1],[-1,  1,  1]]).reshape(2,3)\n",
    "dummyNN.b = np.array([0,0,0]).reshape(1,3)\n",
    "\n",
    "x_h = np.array([[2,-1,1,-1],[1,1,1,1]]).reshape(2,4).T\n",
    "y_h = np.array([1,1,1,1])\n",
    "y_pred_h = dummyNN.propagate(x_h)\n",
    "\n",
    "dummyNN.back_propagate(x_h, y_pred_h, y_h)\n",
    "\n",
    "grad_w_exp = np.array([[ 0.28053421, -0.00450948, -0.27602473],\n",
    "                       [ 0.17666947, -0.60619918,  0.42952972]])\n",
    "grad_b_exp = np.array([[0.17666947,-0.60619918,0.42952972]])\n",
    "np.testing.assert_array_almost_equal(dummyNN.grad_w,grad_w_exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(dummyNN.grad_b,grad_b_exp, decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0caf9-4c0b-4a97-b333-47835d8804b1",
   "metadata": {},
   "source": [
    "### Unit Test for cost_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc857c23-e503-4c9d-8aba-5736979047dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy instance\n",
    "size_out = 3\n",
    "dummyNN = NeuralNetwork(size_out)\n",
    "\n",
    "y_h = np.array([1])\n",
    "y_pred_h = np.array([0.04742587,0.95257413]).reshape(1,2)\n",
    "cost_h = dummyNN.cost_funct(y_pred_h, y_h)\n",
    "cost_exp = 0.04858735\n",
    "np.testing.assert_almost_equal(cost_h,cost_exp,decimal=8)\n",
    "\n",
    "y_h = np.array([1,1,1,0])\n",
    "y_pred_h = np.array([[1.79862100e-02, 6.69285092e-03, 4.74258732e-02, 9.99088949e-01],\n",
    "                  [9.82013790e-01, 9.93307149e-01, 9.52574127e-01, 9.11051194e-04]]).T\n",
    "cost_h = dummyNN.cost_funct(y_pred_h, y_h)\n",
    "cost_exp = 0.01859102\n",
    "np.testing.assert_almost_equal(cost_h,cost_exp,decimal=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597bf615-f73b-4de7-a317-b72c37b98a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
